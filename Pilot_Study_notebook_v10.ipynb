{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNoAIOiTfGT8tvXOPL5J03p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Axiom-G/Axiom-G/blob/main/Pilot_Study_notebook_v10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pilot study: Measuring Psychological Variables in Text: a tutorial\n",
        "\n",
        "Authors: Goddard, A. & Gillespie, A.\n",
        "\n",
        "This notebook is for replicating the results of a pilot study for measuring misunderstandings in online dialogue.\n",
        "\n",
        "For both understanding and misunderstanding:\n",
        "\n",
        "1. A dictionary based classifier \n",
        "\n",
        "2. A ML classifier (using Google's BERT transformer model - see [Devlin et al., 2019](https://arxiv.org/abs/1810.04805v1)) \n",
        "\n",
        "All four classifiers are binary (e.g. misunderstanding/not misunderstanding) and are validated using general accuracy statistics.\n",
        "\n",
        "*NOTE: PLEASE RUN FINAL CELL TO INSTALL ALL PACKAGES FOR THIS TUTORIAL* "
      ],
      "metadata": {
        "id": "GhyPknkNvSg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Clean and Load Data\n",
        "\n",
        "In this section we load the data using Google Drive and perform a few very basic cleaning tasks. As the manually coded data has already been cleaned – and we want a raw text input – not much needs to be done to the data. "
      ],
      "metadata": {
        "id": "cm8aj2yAPPHn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P61S7uGwvKdg"
      },
      "outputs": [],
      "source": [
        "# for setting directory\n",
        "import os \n",
        "from pathlib import Path\n",
        "# for mathematical / vectoral operations\n",
        "import numpy as np\n",
        "# for loading google drive\n",
        "from google.colab import drive\n",
        "# for loading dataframes and spreadsheets (.csv)\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load google drive\n",
        "drive.mount('/content/drive/')\n",
        "# pick path to data\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Datasets/_PhDdata\")\n",
        "# load manually coded dataset\n",
        "adf = pd.read_csv('Final_TurnCodedDataset_July2022_v2.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sBqqjc72PrVp",
        "outputId": "bb461e2d-ed1f-4619-8c28-60f2d25a57a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select relevant columns for analysis - \"M_C\" indicates misunderstandings and \"U_C\" understandings\n",
        "adf = adf[[\"turn_id\", \"group\", \"turn\", \"author\", \"text\", \"M_C\", \"U_C\"]]"
      ],
      "metadata": {
        "id": "E-Xz5dkePvRy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete any rows with no text\n",
        "adf = adf[adf[\"text\"] != \"\"]\n",
        "# delte all values for misunderstanding not equal to zero or one\n",
        "adf = adf[adf[\"M_C\"].isin([0,1])]\n",
        "# delte all values for understanding not equal to zero or one\n",
        "adf = adf[adf[\"U_C\"].isin([0,1])]"
      ],
      "metadata": {
        "id": "fH-DSp9lQC9n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect dataframe\n",
        "adf.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "OathLsfKQSOI",
        "outputId": "76012141-494c-4ed6-db63-bb91b3fd4f28"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  turn_id            group    turn     author  \\\n",
              "0  162942  Twitter_g_34679  turn_1     153653   \n",
              "1  162941  Twitter_g_34679  turn_2  AskTarget   \n",
              "2    8427         Wiki8427  turn_1    Jayy008   \n",
              "3    8428         Wiki8427  turn_2    Jayy008   \n",
              "4    8429         Wiki8427  turn_3    Jayy008   \n",
              "\n",
              "                                                text  M_C  U_C  \n",
              "0  @AskTarget messaged you again as the replaceme...  0.0  0.0  \n",
              "1  @153653 Thank you for taking the time to reach...  0.0  1.0  \n",
              "2  Another thing, please read [[Wikipedia:GOODCHA...  0.0  0.0  \n",
              "3  That was my fault with the Ballads, I called i...  1.0  0.0  \n",
              "4  Also, if you have a problem with my edits, rep...  0.0  0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88bacfb5-a2e8-495b-9c21-52bf91e4098f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>turn_id</th>\n",
              "      <th>group</th>\n",
              "      <th>turn</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>M_C</th>\n",
              "      <th>U_C</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>162942</td>\n",
              "      <td>Twitter_g_34679</td>\n",
              "      <td>turn_1</td>\n",
              "      <td>153653</td>\n",
              "      <td>@AskTarget messaged you again as the replaceme...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>162941</td>\n",
              "      <td>Twitter_g_34679</td>\n",
              "      <td>turn_2</td>\n",
              "      <td>AskTarget</td>\n",
              "      <td>@153653 Thank you for taking the time to reach...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8427</td>\n",
              "      <td>Wiki8427</td>\n",
              "      <td>turn_1</td>\n",
              "      <td>Jayy008</td>\n",
              "      <td>Another thing, please read [[Wikipedia:GOODCHA...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8428</td>\n",
              "      <td>Wiki8427</td>\n",
              "      <td>turn_2</td>\n",
              "      <td>Jayy008</td>\n",
              "      <td>That was my fault with the Ballads, I called i...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8429</td>\n",
              "      <td>Wiki8427</td>\n",
              "      <td>turn_3</td>\n",
              "      <td>Jayy008</td>\n",
              "      <td>Also, if you have a problem with my edits, rep...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88bacfb5-a2e8-495b-9c21-52bf91e4098f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88bacfb5-a2e8-495b-9c21-52bf91e4098f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88bacfb5-a2e8-495b-9c21-52bf91e4098f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dictionary classifier\n",
        "\n",
        "This dictionary classifier is built using SpaCy's rule-based pattern matcher: https://spacy.io/api/matcher/ \n",
        "\n",
        "First, we load in the dictionary and relevant packages. We also specify a Python dict where keys = target word and values = synonyms for words. These will then be used to create augmented dictionary items, generating a new sentence for each synonym:\n",
        "\n",
        "\n",
        "> `\"I didn't mean\"` becomes: `[\"I didn't say\", \"I didn't intent\", \"I didn't try\", etc.] `\n",
        "\n",
        "These are then converted to (very basic) SpaCy patterns:\n",
        "\n",
        "> `\"I get it.\"` becomes: `{{\"LOWER\":\"I\"}, {\"LOWER\": \"get\"}, {\"LOWER\":\"it\"}, {\"IS_PUNCT\":True}}`\n",
        "\n",
        "The matcher then counts, for each turn in the dialogue, all occurences of the dictionary's items (now SpaCy patterns). "
      ],
      "metadata": {
        "id": "623duM5jQVay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SpaCy's rule based matcher\n",
        "from spacy.matcher import Matcher\n",
        "# SpaCy's medium language model \n",
        "import en_core_web_md\n",
        "# for counting values in a list/array\n",
        "from collections import Counter \n",
        "# for displaying remaining time in a loading bar\n",
        "from tqdm import tqdm \n",
        "# for accuracy statistics\n",
        "from sklearn.metrics import classification_report\n",
        "# load nlp() from SpaCy\n",
        "nlp = en_core_web_md.load()"
      ],
      "metadata": {
        "id": "lP87NErr9mHD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define synonyms for augmenting existing list of dictionary items\n",
        "target_dict = {\"mean\":[\"say\",\"intend\",\"try\"],\n",
        "               \"saying\":[\"meaning\", \"intending\", \"speaking\", \"talking\", \"trying\", \"saying\"],\n",
        "               \"really\":[\"actually\"],\n",
        "               \"understand\":[\"get\",\"comprehend\",\"grasp\", \"realize\", \"see\", \"imagine\"],\n",
        "               \"expect\":[\"want\"],\n",
        "               \"knew\":[\"had known\", \"had foreseen\"],\n",
        "               \"feel\":[\"think\",\"believe\",\"intend\", \"want\",\"mean\"],\n",
        "               \"realize\":[\"get\",\"comprehend\",\"grasp\",\"understand\",\"have any idea\", \"know\", \"have knowledge of\"],\n",
        "               \"meant\":[\"intended\", \"believed\", \"thought\", \"said\"],\n",
        "               \"said\":[\"meant\", \"believed\", \"thought\", \"wrote\", \"written\", \"spoke\"],\n",
        "               \"misunderstood\":[\"miscomprehend\", \"misconstured\"],\n",
        "               \"completely:\": [\"absolutely\", \"actually\", \"also\", \"apparently\", \"basically\", \"clearly\",\n",
        "               \"definitely\", \"especially\", \"essentially\", \"even\", \"extremely\",\n",
        "               \"generally\", \"hardly\", \"so\", \"indeed\", \"mostly\", \"particularly\", \"presently\",\n",
        "               \"primarily\", \"principally\", \"probably\", \"prolly\", \"relatively\", \"somewhat\", \n",
        "               \"soo*\", \"totally\", \"truly\", \"ultimately\", \"specifically\", \"totally\",\n",
        "               \"vastly\", \"undeniably\", \"undoubtedly\", \"definitively\", \"evidently\",\n",
        "               \"fundamentally\", \"infallibly\", \"irrefutably\", \"necessarily\", \"obviously\",\n",
        "               \"positively\", \"100%\", \"complete\", \"total\", \"absolute\", \"full\"],\n",
        "               \"ok\":[\"alright\", \"fair\", \"my mistake\", \"my bad\", \"m\"],\n",
        "               \"know\":[\"have knowledge of\", \"get\",\"comprehend\",\"grasp\",\"understand\",\"have any idea\", \"realize\"],\n",
        "               \"issue\":[\"problem\", \"warning\"],\n",
        "               \"thought\":[\"believed\", \"intended\", \"said\", \"meant\", \"perceived\"],\n",
        "               \"serious\":[\"genuine\",\"honest\",\"truthful\",\"real\"],\n",
        "               \"problem\":[\"issue\", \"warning\"],\n",
        "               \"actually\":[\"really\"],\n",
        "               \"think\":[\"feel\", \"intend\"],\n",
        "               \"offend\": [\"upset\", \"hurt\"],\n",
        "               \"don't\":[\"do not\"],\n",
        "               \"you've\": [\"you have\"],\n",
        "               \"doesn't\":[\"does not\"],\n",
        "               \"you're\": [\"you are\"],\n",
        "               \"I'm\":[\"I am\"],\n",
        "               \"hadn't\":[\"had not\"],\n",
        "               \"I'd\":[\"I had\"],\n",
        "               \"hell\":[\"fuck\"],\n",
        "               \"that's\":[\"that is\"],\n",
        "               \"clarification\":[\"explanation\", \"explanation\", \"answer\"],\n",
        "               \"clarifying\": [\"explaining\", \"showing\", \"demonstrating\"],\n",
        "               \"clarifies\": [\"clears\", \"explains\", \"clarified\", \"cleared\", \"explained\"],\n",
        "               \"thanks\": [\"thank you\"],\n",
        "               \"problem\": [\"issue\", \"issues\", \"grievance\", \"bad experience\", \"bad time\", \"bad treatment\", \"poor experience\", \"poor time\", \"poor treatment\"],\n",
        "               \"difficult\":[\"hard\", \"painful\",\"unfair\",\"unjust\",\"awful\", \"terrible\", \"dreadful\",\"catastrophic\",\"scary\",\"upsetting\"],\n",
        "               \"point\":[\"argument\", \"position\",\"perspective\", \"response\"]}"
      ],
      "metadata": {
        "id": "jmRTX7M5QlU4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dictionaries:\n",
        "dicts = pd.read_csv(\"Dictionaries - IPA (v21).csv\")"
      ],
      "metadata": {
        "id": "Lt_zwkyG9hUj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Prepare data & dictionary items for analysis"
      ],
      "metadata": {
        "id": "n2ikb0L9QmDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make dictionaries into lists and delete any missing values\n",
        "# Understanding\n",
        "und_dict = dicts[\"Understanding\"].to_list()\n",
        "und_dict = [x for x in und_dict if str(x) != \"nan\"]\n",
        "# Misunderstanding\n",
        "mis_dict = dicts[\"Misunderstanding\"].to_list()\n",
        "mis_dict = [x for x in mis_dict if str(x) != \"nan\"]\n",
        "print(\"there are {} items in the understanding dictionary.\".format(len(und_dict)))\n",
        "print(\"there are {} items in the misunderstanding dictionary.\".format(len(mis_dict)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jnlLGbrJQc-L",
        "outputId": "cbf07550-21c3-414f-8a3c-dbc511180a9c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there are 69 items in the understanding dictionary.\n",
            "there are 135 items in the misunderstanding dictionary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create function to map on synonyms\n",
        "def dictionary_augmenter(sentence, target_dict):\n",
        "  # create output list\n",
        "  new_sentences = []\n",
        "  # iterate over dictionary keys and values\n",
        "  for k,v in target_dict.items():\n",
        "    # if the word (key) is in the target sentence\n",
        "    if k in sentence:\n",
        "      # for every synonym in the values\n",
        "      for v_ in v:\n",
        "        # create a new sentence\n",
        "        new_sentences.append(sentence.replace(k, v_))\n",
        "  # return output      \n",
        "  return new_sentences"
      ],
      "metadata": {
        "id": "ElCpLuti-vmn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# augment the understanding dictionary by looping the new function over every sentence\n",
        "und_aug = [dictionary_augmenter(s, target_dict) for s in und_dict] \n",
        "# flatten list of lists [[x,y,z],[i,j,k]] --> [x,y,z,i,j,k]\n",
        "und_aug = [y for x in und_aug for y in x]\n",
        "print(\"{} new items were generated for the understanding dictionary.\".format(len(und_aug)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zY5z8RqP_MFH",
        "outputId": "30d098e1-0a2e-4693-8cb1-fd1912c03671"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "279 new items were generated for the understanding dictionary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# join the new dictionary with the old one\n",
        "und_fDict = und_aug + und_dict\n",
        "# keep only unique values\n",
        "und_fDict = list(set(und_fDict))\n",
        "print(\"Final understanding dictionary: {} items.\".format(len(und_fDict)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TZwmQNv9_n07",
        "outputId": "c045dce3-325a-4170-f0a6-4b7a82b07362"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final understanding dictionary: 343 items.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# repeat for misunderstandings\n",
        "mis_aug = [dictionary_augmenter(s, target_dict) for s in mis_dict]\n",
        "mis_aug = [y for x in mis_aug for y in x]\n",
        "print(\"{} new items were generated for the misunderstanding dictionary.\".format(len(mis_aug)))\n",
        "mis_fDict = mis_aug + mis_dict\n",
        "mis_fDict = list(set(mis_fDict))\n",
        "print(\"Final misunderstanding dictionary: {} items.\".format(len(mis_fDict)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FNJtJW2p__ZZ",
        "outputId": "f8d48dd9-a990-40fe-df7f-c645db59bb34"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "544 new items were generated for the misunderstanding dictionary.\n",
            "Final misunderstanding dictionary: 614 items.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create SpaCy patterns for each list of items\n",
        "\n",
        "def createPattern(sentence, # expects string\n",
        "                  nlp): # expects SpaCy NLP \n",
        "  # output list\n",
        "  out_pat = []\n",
        "  # create spacy document from sentence\n",
        "  doc = nlp(sentence)\n",
        "  # for every token in the document\n",
        "  for t in doc:\n",
        "    # let x be the lowercase token\n",
        "    x = t.text.lower()\n",
        "    # if the token is punctuation:\n",
        "    if t.dep_ == \"punct\":\n",
        "      # add punctuation pattern to output list\n",
        "      out_pat.append({\"IS_PUNCT\":True})\n",
        "    else:\n",
        "      # if not punctuation, return the pattern for a lowercase word\n",
        "      out_pat.append({\"LOWER\":x})\n",
        "  return out_pat"
      ],
      "metadata": {
        "id": "leE2gLYPAZlk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output list for misunderstanding patterns\n",
        "misPatterns = []\n",
        "# for each phrase in the dictionary\n",
        "for x in mis_fDict: \n",
        "  # create pattern from phrase and add to output list\n",
        "  misPatterns.append(createPattern(x, nlp))\n",
        "\n",
        "# repeat for understanding patterns\n",
        "undPatterns = []\n",
        "for x in und_fDict:\n",
        "  undPatterns.append(createPattern(x,nlp))"
      ],
      "metadata": {
        "id": "NYCDwkLeBRep"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Run dictionary"
      ],
      "metadata": {
        "id": "O6hgpjIvQrnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function for generating pattern matches\n",
        "\n",
        "def create_matches(df, # expects dataframe\n",
        "                   patterns, # expect list of patterns\n",
        "                   patterns_tag, # expects string - variable name\n",
        "                   nlp, # expects SpaCy nlp()\n",
        "                   text_col = \"text\"):\n",
        "  # create SpaCy documents for each text in text column\n",
        "  df[\"doc\"] = [nlp(d) for d in df[text_col]]\n",
        "  # initiate the pattern matcher\n",
        "  matcher = Matcher(nlp.vocab)\n",
        "  # add patterns to the matcher\n",
        "  matcher.add(patterns_tag, patterns)\n",
        "  # output list\n",
        "  output = []\n",
        "  # for document in document column\n",
        "  for doc in tqdm(df.doc):\n",
        "    # create matches\n",
        "    matches = matcher(doc)\n",
        "    # count matches\n",
        "    counts = Counter(element[0] for element in matches)\n",
        "    # update output list with frequency counts for document\n",
        "    value = None\n",
        "    for entry in counts:\n",
        "        value = counts[entry]\n",
        "    output.append(value)\n",
        "  # create output column of matches\n",
        "  df[patterns_tag] = output\n",
        "  # fill no matches (nan) with zero\n",
        "  df[patterns_tag] = df[patterns_tag].fillna(0)\n",
        "  # return dataframe\n",
        "  return df\n",
        "\n",
        "# function for making a frequency vector binary \n",
        "def binaryMaker(list_):\n",
        "  out = []\n",
        "  for x in list_:\n",
        "    if x > 0:\n",
        "      out.append(1)\n",
        "    else:\n",
        "      out.append(0)\n",
        "  return out"
      ],
      "metadata": {
        "id": "4UGFsu4GQvgb"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create misunderstanding results\n",
        "adf = create_matches(adf, misPatterns, \"M_D\", nlp)\n",
        "adf[\"M_D\"] = binaryMaker(adf[\"M_D\"].tolist())\n",
        "print(\"there are {x} misunderstanding dictionary hits for {n} turns in the dataset\".format(x = sum(adf[\"M_D\"]), n = len(adf[\"M_D\"])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6gmrS39RI3xe",
        "outputId": "00e88b68-c6e3-4e31-ef76-cdd3ea54ae56"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4032/4032 [00:05<00:00, 698.17it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there are 44 misunderstanding dictionary hits for 4032 turns in the dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create understanding results\n",
        "adf = create_matches(adf, undPatterns, \"U_D\", nlp)\n",
        "adf[\"U_D\"] = binaryMaker(adf[\"U_D\"].tolist())\n",
        "print(\"there are {x} understanding dictionary hits for {n} turns in the dataset\".format(x = sum(adf[\"U_D\"]), n = len(adf[\"U_D\"])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DRjlIrjtI8Bn",
        "outputId": "c4db41f4-15f1-47c1-b542-1f07dc3894a6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4032/4032 [00:01<00:00, 2347.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there are 242 understanding dictionary hits for 4032 turns in the dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Validate model\n",
        "\n",
        "We use the following statistics for assessing the accuracy of the classifiers:\n",
        "\n",
        "*   Accuracy: the ratio of correct predictions \n",
        "*   Precision: the ratio of true positives to true positives and false positives\n",
        "*   Recall: the ratio of true positives to true positives and false negatives\n",
        "*   F1 score: harmonic mean of precision and recall\n"
      ],
      "metadata": {
        "id": "EW_In6QRQukr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Misunderstanding classification report\n",
        "print(classification_report(adf[\"M_C\"].tolist(), adf[\"M_D\"].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "c4Vv9ugSQrP8",
        "outputId": "1e61a3a2-0de6-4e36-d7de-eba4881d19b0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.99      0.96      3723\n",
            "         1.0       0.50      0.07      0.12       309\n",
            "\n",
            "    accuracy                           0.92      4032\n",
            "   macro avg       0.71      0.53      0.54      4032\n",
            "weighted avg       0.90      0.92      0.90      4032\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Understanding classification report\n",
        "print(classification_report(adf[\"U_C\"].tolist(), adf[\"U_D\"].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AFsEyM0WJmoe",
        "outputId": "f1a50b07-8c41-4f95-823b-7cc4563bcc9e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.98      0.92      3322\n",
            "         1.0       0.72      0.25      0.37       710\n",
            "\n",
            "    accuracy                           0.85      4032\n",
            "   macro avg       0.79      0.61      0.64      4032\n",
            "weighted avg       0.83      0.85      0.82      4032\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Machine learning classifier\n",
        "\n",
        "We use the package ktrain: https://github.com/amaiya/ktrain for using Google's Bidirectional Encoder Representations from Transformer (BERT) model, originally introduced by Devlin and colleagues (2019): https://github.com/google-research/bert/blob/master/README.md \n",
        "\n",
        "BERT is a deep learning transformer model trained on large quantities of text data. Unlike classic machine learning, deep learning requires little to no feature engineering. Instead, it uses auto-encoders to generate features from the text. BERT's transformers provide features that (in the simplest of terms) have been trained in a large dataset. These can be transferred through the transformer model and applied to a new natural language processing task (in this case, binary classification). "
      ],
      "metadata": {
        "id": "BwpQcbLOQe5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ktrain: \n",
        "# See also: https://colab.research.google.com/drive/1ixOZTKLz4aAa-MtC6dy_sAvc9HujQmHN#scrollTo=Y8hIFvooF4vc\n",
        "# and: https://medium.com/towards-data-science/bert-text-classification-in-3-lines-of-code-using-keras-264db7e7a358\n",
        "# and: https://towardsdatascience.com/ktrain-a-lightweight-wrapper-for-keras-to-help-train-neural-networks-82851ba889c \n",
        "import ktrain\n",
        "from ktrain import text\n",
        "\n",
        "# for splitting dataset into train and test\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "_bSjpr4EQ1bV"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Misunderstanding classifier\n",
        "### 3.1.1 Prepare data\n",
        "\n",
        "The first step is to split our data into:\n",
        "\n",
        "\n",
        "*   A training set (here 70% of the coded data)\n",
        "*   A validation set (here 30% of the coded data).\n",
        "\n",
        "The model only learns using the training set and we test its accuracy using the validation set. This ensures that validation is only done using new data  not  previously \"seen\" by the model. This prevents the model from overfitting and learning only the precise training documents coded manually as a target variable."
      ],
      "metadata": {
        "id": "pOzzvbWhQ14u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split train dataset into train, validation and test sets\n",
        "# X = Texts\n",
        "X = adf['text'].copy()\n",
        "# Y1 = Understanding codes\n",
        "y_u = adf['U_C'].astype(int).copy()\n",
        "#Y2 = Misunderstanding codes\n",
        "y_m = adf['M_C'].astype(int).copy()\n",
        "\n",
        "# MISUNDERSTANDING test and train vectors - 30% test size and stratified for coding distribution\n",
        "# X_train = list of texts in training; X_test = list of texts for validation\n",
        "# y_train = list of codes for training; y_test = list of codes for validation\n",
        "X_trainM, X_testM, y_trainM, y_testM = train_test_split(X, y_m, test_size=0.30, random_state=10, stratify=y_m)\n",
        "\n",
        "# Create train dataframe, ensuring codes are integers for the ktrain.text function\n",
        "MisTraindf = pd.DataFrame({\"text\": X_trainM, \"Misunderstanding\": [int(x) for x in y_trainM]})\n",
        "# Create validation dataframe, ensuring codes are integers for the ktrain.text function\n",
        "MisValdf = pd.DataFrame({\"text\": X_testM, \"Misunderstanding\": [int(x) for x in y_testM]})\n",
        "\n",
        "# Create objects ready for ktrain learner object (train/test sets + preprocessing object)\n",
        "(x_train_m,  y_train_m), (x_test_m, y_test_m), preproc = text.texts_from_df(train_df = MisTraindf, # training df\n",
        "                                                                            text_column = \"text\",\n",
        "                                                                            label_columns = [\"Misunderstanding\"], \n",
        "                                                                            val_df = MisValdf, # validation df\n",
        "                                                                            preprocess_mode='bert', # preprocessing mode for feature embeddings - Google's BERT\n",
        "                                                                            maxlen=350, # this is the max number of words for a document\n",
        "                                                                            max_features=35000) # size of network\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "u63XwQvWQ8PR",
        "outputId": "c67ed411-7b0d-46b1-f3d7-70210d3f26ac"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['not_Misunderstanding', 'Misunderstanding']\n",
            "      not_Misunderstanding  Misunderstanding\n",
            "1256                   1.0               0.0\n",
            "3281                   1.0               0.0\n",
            "2646                   1.0               0.0\n",
            "3054                   1.0               0.0\n",
            "276                    1.0               0.0\n",
            "['not_Misunderstanding', 'Misunderstanding']\n",
            "      not_Misunderstanding  Misunderstanding\n",
            "481                    1.0               0.0\n",
            "3726                   1.0               0.0\n",
            "3715                   1.0               0.0\n",
            "2768                   1.0               0.0\n",
            "1424                   1.0               0.0\n",
            "preprocessing train...\n",
            "language: en\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "done."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Multi-Label? False\n",
            "preprocessing test...\n",
            "language: en\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "done."
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.2 Train model"
      ],
      "metadata": {
        "id": "mZN05tPJQ85_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the bert model and create a \"learner\" object\n",
        "\n",
        "# load a BERT text classifier and specify the training data - \"preproc\" is the relevant bert preprocessing \n",
        "model = text.text_classifier('bert', train_data=(x_train_m, y_train_m), preproc=preproc)\n",
        "\n",
        "# Load the ktrain \"learner\" object that primes the algorithm - Batch size should be set according to the GPU capabilities: https://github.com/google-research/bert/blob/master/README.md \n",
        "learner = ktrain.get_learner(model, train_data=(x_train_m, y_train_m), batch_size=6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4FZmy-tbQ9yd",
        "outputId": "657de008-9699-4c9b-a6ab-7b2b53488791"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Multi-Label? False\n",
            "maxlen is 350\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run the model - in BERT Paper (Devlin et al., 2019), they recommend 5e-5, 3e-5, or 2e-5. However, this can also be found using learner.lr_find() or learner.lr_plot()\n",
        "# We run on four iterations\n",
        "learner.fit_onecycle(2e-5, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5m_u5T91M6ST",
        "outputId": "a8d044fb-f4eb-4397-b313-700e6646035a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "begin training using onecycle policy with max lr of 2e-05...\n",
            "Epoch 1/4\n",
            "471/471 [==============================] - 196s 349ms/step - loss: 0.3028 - accuracy: 0.9047\n",
            "Epoch 2/4\n",
            "471/471 [==============================] - 164s 348ms/step - loss: 0.2117 - accuracy: 0.9238\n",
            "Epoch 3/4\n",
            "471/471 [==============================] - 164s 348ms/step - loss: 0.1338 - accuracy: 0.9483\n",
            "Epoch 4/4\n",
            "471/471 [==============================] - 164s 349ms/step - loss: 0.0225 - accuracy: 0.9933\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f83a62eef50>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.3 Validate model"
      ],
      "metadata": {
        "id": "EIytIL2cQ-Dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate the model\n",
        "learner.validate(val_data=(x_test_m, y_test_m))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "L1slDCOMRIiX",
        "outputId": "124c2a43-50e8-4ca6-8f76-e649b8b84653"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.98      0.96      1117\n",
            "           1       0.54      0.30      0.39        93\n",
            "\n",
            "    accuracy                           0.93      1210\n",
            "   macro avg       0.74      0.64      0.67      1210\n",
            "weighted avg       0.91      0.93      0.92      1210\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1093,   24],\n",
              "       [  65,   28]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Understandings classifier"
      ],
      "metadata": {
        "id": "VPRIk4ZmRJE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preperation\n",
        "\n",
        "X_trainU, X_testU, y_trainU, y_testU = train_test_split(X, y_u, test_size=0.30, random_state=10, stratify=y_m)\n",
        "\n",
        "\n",
        "UndTraindf = pd.DataFrame({\"text\": X_trainU, \"Understanding\": [int(x) for x in y_trainU]})\n",
        "UndValdf = pd.DataFrame({\"text\": X_testU, \"Understanding\": [int(x) for x in y_testU]})\n",
        "\n",
        "(x_train_u,  y_train_u), (x_test_u, y_test_u), preproc = text.texts_from_df(train_df = UndTraindf,\n",
        "                                                                            text_column = \"text\",\n",
        "                                                                            label_columns = [\"Understanding\"],\n",
        "                                                                            val_df = UndValdf,\n",
        "                                                                            preprocess_mode='bert',\n",
        "                                                                            maxlen=350, \n",
        "                                                                            max_features=35000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "s96p4eaXRNgu",
        "outputId": "cedb0248-a3a1-4248-b950-55bc67e95d95"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['not_Understanding', 'Understanding']\n",
            "      not_Understanding  Understanding\n",
            "1256                1.0            0.0\n",
            "3281                1.0            0.0\n",
            "2646                1.0            0.0\n",
            "3054                1.0            0.0\n",
            "276                 1.0            0.0\n",
            "['not_Understanding', 'Understanding']\n",
            "      not_Understanding  Understanding\n",
            "481                 1.0            0.0\n",
            "3726                1.0            0.0\n",
            "3715                1.0            0.0\n",
            "2768                1.0            0.0\n",
            "1424                1.0            0.0\n",
            "preprocessing train...\n",
            "language: en\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "done."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Multi-Label? False\n",
            "preprocessing test...\n",
            "language: en\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "done."
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "model_und = text.text_classifier('bert', train_data=(x_train_u, y_train_u), preproc=preproc)\n",
        "learner_und = ktrain.get_learner(model_und, train_data=(x_train_u, y_train_u), batch_size=6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4lAGUbfuN9Hu",
        "outputId": "b3f6894b-592e-49a7-e0cb-3620d7dd196e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Multi-Label? False\n",
            "maxlen is 350\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run model\n",
        "learner_und.fit_onecycle(2e-5, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HU9vZ_9ZN_oY",
        "outputId": "527375f1-6d4d-4cc9-f746-c92daaae0b72"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "begin training using onecycle policy with max lr of 2e-05...\n",
            "Epoch 1/4\n",
            "471/471 [==============================] - 192s 349ms/step - loss: 0.3243 - accuracy: 0.8650\n",
            "Epoch 2/4\n",
            "471/471 [==============================] - 164s 349ms/step - loss: 0.2037 - accuracy: 0.9185\n",
            "Epoch 3/4\n",
            "471/471 [==============================] - 164s 348ms/step - loss: 0.1317 - accuracy: 0.9511\n",
            "Epoch 4/4\n",
            "471/471 [==============================] - 164s 349ms/step - loss: 0.0504 - accuracy: 0.9819\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f832073ba50>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate the model\n",
        "learner_und.validate(val_data=(x_test_u, y_test_u))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wUc44aqsOFYY",
        "outputId": "105ae157-c3ef-4fea-edb3-badf09b274b9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95       991\n",
            "           1       0.78      0.79      0.79       219\n",
            "\n",
            "    accuracy                           0.92      1210\n",
            "   macro avg       0.87      0.87      0.87      1210\n",
            "weighted avg       0.92      0.92      0.92      1210\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[941,  50],\n",
              "       [ 45, 174]])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Testing understanding model\n",
        "\n",
        "The machine learning models can be used to predict a classification for any new document. Here we use our best performing understandings model as an example."
      ],
      "metadata": {
        "id": "UPQM0yfuOIQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a predictor tool\n",
        "predictor = ktrain.get_predictor(learner_und.model, preproc)"
      ],
      "metadata": {
        "id": "6Gfm_1ROOGru"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check categories\n",
        "predictor.get_classes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5kR9fQWzOOxI",
        "outputId": "de8e097f-9ed5-43ff-b229-e33322d935ef"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['not_Understanding', 'Understanding']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.predict(\"I don't get what you're saying\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vYyLz7IiOQ0A",
        "outputId": "a129848e-d66a-49f9-ecbd-a8581c481ba5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'not_Understanding'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.predict(\"Thank you so much\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oblna11NOSHW",
        "outputId": "6a9e17af-3dd1-4338-8ce6-0d50651153ee"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Understanding'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.predict(\"Ah, I get it now\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ad_10iquOTRv",
        "outputId": "ae53d91c-115c-4767-d55e-bc92ce765b63"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Understanding'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.predict(\"I don't get it dude\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_2ynY_sSOUi0",
        "outputId": "4274d4c0-499d-47ea-89b6-91ff98b8cca4"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'not_Understanding'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUN ME FIRST"
      ],
      "metadata": {
        "id": "BjywbsGCOZB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md\n",
        "!pip install ktrain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MXGcEQmWvlij",
        "outputId": "635a9e1a-77f9-4c46-d0e9-a080a76af8aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.0/en_core_web_md-3.4.0-py3-none-any.whl (42.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.8 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ktrain\n",
            "  Downloading ktrain-0.31.7.tar.gz (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.2.2)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.3.5)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ktrain) (21.3)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 37.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.42.1)\n",
            "Collecting cchardet\n",
            "  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
            "\u001b[K     |████████████████████████████████| 263 kB 43.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.0.4)\n",
            "Collecting syntok>1.3.3\n",
            "  Downloading syntok-1.4.4-py3-none-any.whl (24 kB)\n",
            "Collecting transformers==4.17.0\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 50.6 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 51.0 MB/s \n",
            "\u001b[?25hCollecting keras_bert>=0.86.0\n",
            "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
            "Collecting whoosh\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[K     |████████████████████████████████| 468 kB 43.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (4.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (1.21.6)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (4.64.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 46.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0->ktrain) (4.1.1)\n",
            "Collecting keras-transformer==0.40.0\n",
            "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
            "Collecting keras-pos-embd==0.13.0\n",
            "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
            "Collecting keras-multi-head==0.29.0\n",
            "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "Collecting keras-layer-normalization==0.16.0\n",
            "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
            "Collecting keras-position-wise-feed-forward==0.8.0\n",
            "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
            "Collecting keras-embed-sim==0.10.0\n",
            "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
            "Collecting keras-self-attention==0.51.0\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->ktrain) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->ktrain) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.17.0->ktrain) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.17.0->ktrain) (7.1.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->ktrain) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->ktrain) (3.1.0)\n",
            "Building wheels for collected packages: ktrain, keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, langdetect, sacremoses\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ktrain: filename=ktrain-0.31.7-py3-none-any.whl size=25312842 sha256=9c84f2ca430b90622bf96e609f001880f447cc603cbedc36297762118405bba4\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/98/8e/ce355dcb92451e85fab93f7ea2da068843e93e703928cd06fb\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=36974b2b306e55958d27aae62bc3216973c4810365a35fcb464a7e4a8701f760\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/e8/45/842b3a39831261aef9154b907eacbc4ac99499a99ae829b06f\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12305 sha256=061377c26fd5d97efb587eb244494d5119f0fe23b06620571a3da578eb88e37f\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/68/26/692ed21edd832833c3b0a0e21615bcacd99ca458b3f9ed571f\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3960 sha256=9f53d44cf48de616706e6c1ce478d73575c008fd825796dfff89fa22cc8ec46b\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/67/b5/d847588d075895281e1cf5590f819bd4cf076a554872268bd5\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=db34e3978ab6d25b7c60cbccebd53564f46bb9672aa8a810231df1d886de30ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/5d/1c/2e619f594f69fbcf8bc20943b27d414871c409be053994813e\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14993 sha256=2e9c0684d8881e2d6d2c80dc12cdd6c33527a34c157597f69bec3dc5f38542aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/aa/3c/9d15d24005179dae08ff291ce99c754b296347817d076fd9fb\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6962 sha256=2013230b35a88a6a22477018998f9ddb0dd3c29e024e3c6f76614a265539b0be\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/c1/a0/dc44fcf68c857b7ff6be9a97e675e5adf51022eff1169b042f\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=d8e99694510b49d1741e6991b034e1a2c5d399a84a9d490b40a52a38834db90e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/75/6f/d42f6e051506f442daeba53ff1e2d21a5f20ef8c411610f2bb\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=c738fceca5ba600004ea68d60f5adaec531b71f9cd4e9af374504d8b35558613\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=27588e33738102eb23b40afa7af0648bdd6dac0fa01de9b1232deb7fdf42bed5\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=e9ac909088c52899de78d4bf4d06ae706ae8c2419e4dff5ad490f9e9b8072a17\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built ktrain keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention langdetect sacremoses\n",
            "Installing collected packages: keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, tokenizers, sacremoses, keras-transformer, huggingface-hub, whoosh, transformers, syntok, sentencepiece, langdetect, keras-bert, cchardet, ktrain\n",
            "Successfully installed cchardet-2.1.7 huggingface-hub-0.9.1 keras-bert-0.89.0 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 ktrain-0.31.7 langdetect-1.0.9 sacremoses-0.0.53 sentencepiece-0.1.97 syntok-1.4.4 tokenizers-0.12.1 transformers-4.17.0 whoosh-2.7.4\n"
          ]
        }
      ]
    }
  ]
}